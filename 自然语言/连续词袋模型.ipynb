{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7989fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import typing_extensions\n",
    "# 设置 matplotlib 支持中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 正确显示负号\n",
    "# 设置随机种子以确保结果可复现\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9afe6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据\n",
    "sentences = [\n",
    "     \"自监督学习减少对标注数据的依赖\",\n",
    "    \"模型微调适应特定领域任务\",\n",
    "    \"评估指标衡量算法性能优劣\",\n",
    "    \"过拟合问题影响模型的泛化能力\"]\n",
    "# 配置模型训练所需的超参数\n",
    "CONFIG = {\n",
    "    \"window_size\": 2,  # 上下文窗口大小\n",
    "    \"embedding_dim\": 16,  # 词向量的维度\n",
    "    \"initial_lr\": 0.1,  # 初始学习率\n",
    "    \"min_epochs\": 200,  # 最小训练轮数\n",
    "    \"patience\": 100,  # 早停机制的耐心值，即允许损失不下降的最大轮数\n",
    "    \"min_delta\": 1e-4,  # 早停机制中损失改善的最小阈值\n",
    "    \"l2_lambda\": 0.001,  # L2正则化系数\n",
    "    \"momentum\": 0.9,  # 动量系数\n",
    "    \"max_grad_norm\": 5.0,  # 梯度裁剪的最大范数\n",
    "    \"batch_size\": 32  # 批量大小\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a39011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences: List[str]) -> Tuple[List[str], Dict, Dict]:\n",
    "    # 将所有句子中的词拆分成单个字符，并存储在一个列表中\n",
    "    words = [word for sent in sentences for word in list(sent)]\n",
    "    vocab = list(set(words))\n",
    "# 返回词汇表、词到索引的映射和索引到词的映射\n",
    "    return vocab, {word: idx for idx, word in enumerate(vocab)}, \\\n",
    "    {idx: word for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f505db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cbow_data(sentences: List[str], window_size: int) -> List[Tuple]:\n",
    "    data = []\n",
    "    for sent in sentences:\n",
    "        # 将句子拆分成单个字符\n",
    "        words = list(sent)\n",
    "        # 遍历句子中的每个词，排除窗口范围内无法构成完整上下文的词\n",
    "        for i in range(window_size, len(words) - window_size):\n",
    "            # 提取当前词的上下文，即窗口内除当前词以外的所有词\n",
    "            context = [words[i + j] for j in range(-window_size, window_size + 1) if j != 0]\n",
    "            # 当前词作为目标词\n",
    "            target = words[i]\n",
    "            # 将上下文和目标词作为一个元组添加到数据列表中\n",
    "            data.append((context, target))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2103b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model:\n",
    "    def __init__(self, vocab_size: int, config: dict):\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = vocab_size\n",
    "        # 超参数配置\n",
    "        self.config = config\n",
    "        # 初始化嵌入层权重，使用 Xavier 初始化方法\n",
    "        self.embed_weights = np.random.randn(vocab_size, config[\"embedding_dim\"]) * np.sqrt(2. / (vocab_size + config[\"embedding_dim\"]))\n",
    "        # 初始化上下文层权重，使用 Xavier 初始化方法\n",
    "        self.context_weights = np.random.randn(config[\"embedding_dim\"], vocab_size) * np.sqrt(2. / (config[\"embedding_dim\"] + vocab_size))\n",
    "        # 初始化嵌入层动量\n",
    "        self.embed_momentum = np.zeros_like(self.embed_weights)\n",
    "        # 初始化上下文层动量\n",
    "        self.context_momentum = np.zeros_like(self.context_weights)\n",
    " # 实现softmax函数，用于将输入转换为概率分布\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        # 为了避免数值溢出，先减去最大值\n",
    "        x = x - np.max(x)\n",
    "        e_x = np.exp(x)\n",
    "        return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70492fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 前向传播和反向传播函数，计算损失和梯度\n",
    "    def forward_backward(self, context_indices: List[int], target_idx: int) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        x = np.zeros(self.vocab_size)\n",
    "        # 将上下文词的索引对应的向量元素累加，并除以上下文词的数量\n",
    "        for idx in context_indices:\n",
    "            x[idx] += 1 / len(context_indices)\n",
    "        # 计算隐藏层向量\n",
    "        h = np.dot(x, self.embed_weights)\n",
    "        # 计算logits\n",
    "        logits = np.dot(h, self.context_weights)\n",
    "        # 计算概率分布\n",
    "        probs = self.softmax(logits)\n",
    "        # 计算损失，包括交叉熵损失和L2正则化项\n",
    "        loss = -np.log(probs[target_idx] + 1e-8) + 0.5 * self.config[\"l2_lambda\"] * (\n",
    "                np.sum(self.embed_weights ** 2) + np.sum(self.context_weights ** 2))\n",
    "        # 计算logits的梯度\n",
    "        d_logits = probs.copy()\n",
    "        d_logits[target_idx] -= 1\n",
    "        # 计算上下文层权重的梯度\n",
    "        grad_context = np.outer(h, d_logits) + self.config[\"l2_lambda\"] * self.context_weights\n",
    "        # 计算隐藏层的梯度\n",
    "        dh = np.dot(self.context_weights, d_logits)\n",
    "        # 计算嵌入层权重的梯度\n",
    "        grad_embed = np.outer(x, dh) + self.config[\"l2_lambda\"] * self.embed_weights\n",
    "        # 返回损失、嵌入层梯度、上下文层梯度和概率分布\n",
    "        return loss, grad_embed, grad_context, probs\n",
    "    # 更新模型参数，使用动量优化器\n",
    "    def update_parameters(self, gradients: Tuple[np.ndarray, np.ndarray], lr: float):\n",
    "        # 解包梯度\n",
    "        grad_embed, grad_context = gradients\n",
    "        # 计算梯度的总范数\n",
    "        total_norm = np.sqrt(np.sum(grad_embed ** 2) + np.sum(grad_context ** 2))\n",
    "        # 如果总范数超过最大范数，则进行梯度裁剪\n",
    "        if total_norm > self.config[\"max_grad_norm\"]:\n",
    "            scale = self.config[\"max_grad_norm\"] / total_norm\n",
    "            grad_embed *= scale\n",
    "            grad_context *= scale\n",
    "  # 更新嵌入层动量\n",
    "        self.embed_momentum = self.config[\"momentum\"] * self.embed_momentum + (1 - self.config[\"momentum\"]) * grad_embed\n",
    "        # 更新上下文层动量\n",
    "        self.context_momentum = self.config[\"momentum\"] * self.context_momentum + (1 - self.config[\"momentum\"]) * grad_context\n",
    "        # 更新嵌入层权重\n",
    "        self.embed_weights -= lr * self.embed_momentum\n",
    "        # 更新上下文层权重\n",
    "        self.context_weights -= lr * self.context_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4cd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练CBOW模型\n",
    "def train_model(sentences: List[str], config: dict) -> Tuple[CBOW_Model, dict, Dict, Dict]:\n",
    "    # 构建词汇表和词索引映射\n",
    "    vocab, word_to_idx, idx_to_word = build_vocab(sentences)\n",
    "    # 创建训练数据\n",
    "    training_data = create_cbow_data(sentences, config[\"window_size\"])\n",
    "    print(f\"生成训练样本数: {len(training_data)}\")\n",
    "    print(f\"词汇表: {vocab}\")\n",
    "    # 初始化CBOW模型\n",
    "    model = CBOW_Model(len(vocab), config)\n",
    "    # 记录训练历史\n",
    "    history = defaultdict(list)\n",
    "    # 初始化最佳损失为正无穷\n",
    "    best_loss = float('inf')\n",
    "    # 初始化早停计数器\n",
    "    no_improve = 0\n",
    "    print(\"\\n开始训练...\")\n",
    "    for epoch in range(10000):\n",
    "        # 学习率衰减\n",
    "        lr = config[\"initial_lr\"] * (0.95 ** (epoch // 50))\n",
    "        # 初始化总损失\n",
    "        total_loss = 0\n",
    "        # 初始化正确预测的样本数\n",
    "        correct = 0\n",
    " # 打乱训练数据\n",
    "        np.random.shuffle(training_data)\n",
    "        for i in range(0, len(training_data), config[\"batch_size\"]):\n",
    "            # 取一个批次的数据\n",
    "            batch = training_data[i:i + config[\"batch_size\"]]\n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "            # 初始化批次梯度\n",
    "            batch_grad_e = np.zeros_like(model.embed_weights)\n",
    "            batch_grad_c = np.zeros_like(model.context_weights)\n",
    "            for context, target in batch:\n",
    "                # 将上下文词转换为索引\n",
    "                context_idx = [word_to_idx[w] for w in context]\n",
    "                # 将目标词转换为索引\n",
    "                target_idx = word_to_idx[target]\n",
    "                # 前向传播和反向传播，计算损失和梯度\n",
    "                loss, grad_e, grad_c, probs = model.forward_backward(context_idx, target_idx)\n",
    "                # 获取预测的索引\n",
    "                predicted_idx = np.argmax(probs)\n",
    "                if predicted_idx == target_idx:\n",
    "                    correct += 1\n",
    "                # 累加梯度\n",
    "                batch_grad_e += grad_e\n",
    "                batch_grad_c += grad_c\n",
    "                # 累加损失\n",
    "                total_loss += loss\n",
    "            # 计算批次平均梯度\n",
    "            batch_grad_e /= len(batch)\n",
    "            batch_grad_c /= len(batch)\n",
    "            # 更新模型参数\n",
    "            model.update_parameters((batch_grad_e, batch_grad_c), lr)\n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / len(training_data)\n",
    "        # 计算准确率\n",
    "        accuracy = correct / len(training_data)\n",
    "        # 记录损失、准确率和学习率\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['acc'].append(accuracy)\n",
    "        history['lr'].append(lr)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1:4d} | Loss: {avg_loss:.4f} | Acc: {accuracy:.2%} | LR: {lr:.5f}\")\n",
    "        if epoch >= config[\"min_epochs\"]:\n",
    "            if avg_loss < best_loss - config[\"min_delta\"]:\n",
    "                # 如果损失有改善，更新最佳损失并重置早停计数器\n",
    "                best_loss = avg_loss\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                # 如果损失没有改善，早停计数器加1\n",
    "                no_improve += 1\n",
    "            if no_improve >= config[\"patience\"]:\n",
    "                print(f\"\\n早停触发：连续 {config['patience']} 轮损失未改善\")\n",
    "                break\n",
    "    print(f\"训练结束，最优损失: {best_loss:.4f}\")\n",
    "    return model, history, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a2b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练结果\n",
    "def visualize_results(model: CBOW_Model, history: dict, idx_to_word: Dict[int, str]):\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    # 绘制训练损失曲线，使用对数尺度\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.semilogy(history['loss'])\n",
    "    plt.title(\"训练损失（对数尺度）\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    # 绘制训练准确率曲线\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['acc'])\n",
    "    plt.title(\"训练准确率\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    # 绘制学习率曲线\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['lr'])\n",
    "    plt.title(\"学习率调度\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.tight_layout()\n",
    "    # 计算词向量的相似度矩阵\n",
    "    vectors = model.embed_weights\n",
    "    sim_matrix = vectors @ vectors.T\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    sim_matrix /= (norms @ norms.T)\n",
    "    # 绘制相似度矩阵热力图\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(sim_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(idx_to_word)), [idx_to_word[i] for i in range(len(idx_to_word))], rotation=90)\n",
    "    plt.yticks(range(len(idx_to_word)), [idx_to_word[i] for i in range(len(idx_to_word))])\n",
    "    plt.title(\"词相似度矩阵\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da2d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 训练模型\n",
    "    trained_model, training_history, word_to_idx, idx_to_word = train_model(sentences, CONFIG)\n",
    "    # 可视化训练结果\n",
    "    visualize_results(trained_model, training_history, idx_to_word)\n",
    "    print(\"\\n示例词向量：\")\n",
    "    for idx in range(len(idx_to_word)):\n",
    "        # 获取词\n",
    "        word = idx_to_word[idx]\n",
    "        # 打印词和对应的词向量\n",
    "        print(f\"{word}: {trained_model.embed_weights[idx].round(4)}\")\n",
    "print(\"主程序\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
