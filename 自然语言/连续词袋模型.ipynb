{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dd5ca2",
   "metadata": {},
   "source": [
    "# 连续词袋模型 (CBOW) 实现\n",
    "\n",
    "本 Notebook 演示如何使用 Python 和 TensorFlow/Keras 构建和训练一个简单的连续词袋模型 (CBOW)。\n",
    "\n",
    "**目标:**\n",
    "\n",
    "*   理解 CBOW 模型的基本原理。\n",
    "*   使用 TensorFlow/Keras 实现 CBOW 模型。\n",
    "*   准备训练数据。\n",
    "*   训练模型并评估其性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb07e9",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae19e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a3cab",
   "metadata": {},
   "source": [
    "## 2. 准备数据\n",
    "\n",
    "首先，我们需要准备用于训练的数据。 在这个例子中，我们使用一个简单的句子。\n",
    "\n",
    "**注意:** 实际应用中，你需要使用更大的语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e45d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# 1. 数据预处理\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = len(word_index) + 1 # +1 for the padding token (if needed)\n",
    "\n",
    "print(f\"词汇表大小: {vocabulary_size}\")\n",
    "print(f\"单词索引: {word_index}\")\n",
    "\n",
    "encoded_sentence = tokenizer.texts_to_sequences([sentence])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9abba",
   "metadata": {},
   "source": [
    "## 3. 创建训练数据 (Skip-grams)\n",
    "\n",
    "CBOW 通过上下文词预测目标词。我们需要生成上下文-目标词对。 这里使用 `skipgrams` 函数，它生成 skip-gram 形式的词对，适用于 CBOW 和 Skip-gram 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef90e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 生成 Skip-gram 数据\n",
    "window_size = 2 # 上下文窗口大小\n",
    "negative_samples = 1.0 # 每个正样本的负样本数\n",
    "\n",
    "couples, labels = skipgrams(encoded_sentence, vocabulary_size, window_size=window_size, negative_samples=negative_samples)\n",
    "\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(f\"生成的词对数量: {len(couples)}\")\n",
    "print(f\"第一个词对: target={word_target[0]}, context={word_context[0]}, label={labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f5b6c",
   "metadata": {},
   "source": [
    "## 4. 构建 CBOW 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a6cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 构建 CBOW 模型\n",
    "embedding_dim = 50 # 词嵌入维度\n",
    "\n",
    "cbow = keras.models.Sequential([\n",
    "    layers.Embedding(vocabulary_size, embedding_dim, input_length=1),\n",
    "    layers.Reshape((embedding_dim,)),\n",
    "    layers.Dense(vocabulary_size, activation='sigmoid') # 使用 softmax 进行多分类, 优化器需要调整\n",
    "])\n",
    "\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520dec2",
   "metadata": {},
   "source": [
    "## 5. 编译和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b11e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 编译模型\n",
    "cbow.compile(optimizer='adam', # Adam 优化器\n",
    "              loss='sparse_categorical_crossentropy', # 适用于多分类问题\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. 训练模型\n",
    "epochs = 100\n",
    "\n",
    "cbow.fit([word_target, word_context], np.array(labels), epochs=epochs, batch_size=256, verbose=0)\n",
    "\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d481ca7",
   "metadata": {},
   "source": [
    "## 6.  评估 (可选) 和使用词嵌入\n",
    "\n",
    "由于数据集非常小，直接评估模型的质量意义不大。  通常，你需要在一个更大的数据集上训练模型，然后评估生成的词嵌入在其他任务上的表现 (例如，词语相似度，文本分类等)。\n",
    "\n",
    "这里展示如何提取词嵌入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f16135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 获取词嵌入\n",
    "embedding_layer = cbow.layers[0]\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"词嵌入矩阵的形状: {embeddings.shape}\") # (vocabulary_size, embedding_dim)\n",
    "\n",
    "# 获取 \"fox\" 的词嵌入\n",
    "word = \"fox\"\n",
    "word_id = word_index[word]\n",
    "embedding_vector = embeddings[word_id]\n",
    "\n",
    "print(f\"'{word}' 的词嵌入: {embedding_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f7059",
   "metadata": {},
   "source": [
    "## 7. 完整代码（整合）\n",
    "\n",
    "为了方便复制，这里提供一个完整的代码版本。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# 1. 数据预处理\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = len(word_index) + 1 # +1 for the padding token (if needed)\n",
    "\n",
    "print(f\"词汇表大小: {vocabulary_size}\")\n",
    "print(f\"单词索引: {word_index}\")\n",
    "\n",
    "encoded_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "# 2. 生成 Skip-gram 数据\n",
    "window_size = 2 # 上下文窗口大小\n",
    "negative_samples = 1.0 # 每个正样本的负样本数\n",
    "\n",
    "couples, labels = skipgrams(encoded_sentence, vocabulary_size, window_size=window_size, negative_samples=negative_samples)\n",
    "\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(f\"生成的词对数量: {len(couples)}\")\n",
    "print(f\"第一个词对: target={word_target[0]}, context={word_context[0]}, label={labels[0]}\")\n",
    "\n",
    "# 3. 构建 CBOW 模型\n",
    "embedding_dim = 50 # 词嵌入维度\n",
    "\n",
    "cbow = keras.models.Sequential([\n",
    "    layers.Embedding(vocabulary_size, embedding_dim, input_length=1),\n",
    "    layers.Reshape((embedding_dim,)),\n",
    "    layers.Dense(vocabulary_size, activation='sigmoid') # 使用 softmax 进行多分类, 优化器需要调整\n",
    "])\n",
    "\n",
    "cbow.summary()\n",
    "\n",
    "# 4. 编译模型\n",
    "cbow.compile(optimizer='adam', # Adam 优化器\n",
    "              loss='sparse_categorical_crossentropy', # 适用于多分类问题\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. 训练模型\n",
    "epochs = 100\n",
    "\n",
    "cbow.fit([word_target, word_context], np.array(labels), epochs=epochs, batch_size=256, verbose=0)\n",
    "\n",
    "print(\"训练完成!\")\n",
    "\n",
    "# 6. 获取词嵌入\n",
    "embedding_layer = cbow.layers[0]\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"词嵌入矩阵的形状: {embeddings.shape}\") # (vocabulary_size, embedding_dim)\n",
    "\n",
    "# 获取 \"fox\" 的词嵌入\n",
    "word = \"fox\"\n",
    "word_id = word_index[word]\n",
    "embedding_vector = embeddings[word_id]\n",
    "\n",
    "print(f\"'{word}' 的词嵌入: {embedding_vector}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd955a",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本 Notebook 演示了如何使用 TensorFlow/Keras 构建和训练一个简单的 CBOW 模型。 请记住，这只是一个基础示例，实际应用中需要更大的数据集和更复杂的模型结构。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
