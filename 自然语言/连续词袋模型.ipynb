{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文连续词袋模型 (CBOW) 实现\n",
    "\n",
    "本 Notebook 演示如何使用 Python 和 TensorFlow/Keras 构建和训练一个中文连续词袋模型 (CBOW)。\n",
    "\n",
    "**目标:**\n",
    "\n",
    "*   理解 CBOW 模型的基本原理。\n",
    "*   使用 TensorFlow/Keras 实现 CBOW 模型。\n",
    "*   准备中文训练数据。\n",
    "*   使用分词工具处理中文文本。\n",
    "*   训练模型并评估其性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "import jieba  # 用于中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备中文数据\n",
    "\n",
    "我们使用一个简单的中文句子作为示例。实际应用中，需要使用更大的中文语料库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"自然语言处理是人工智能的一个重要分支。它涉及计算机对人类语言的理解和生成。\"\n",
    "\n",
    "# 1. 中文分词\n",
    "seg_list = jieba.cut(sentence, cut_all=False)  # 使用精确模式分词\n",
    "segmented_sentence = \" \".join(seg_list)\n",
    "\n",
    "print(f\"分词结果: {segmented_sentence}\")\n",
    "\n",
    "# 2. 数据预处理\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([segmented_sentence])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = len(word_index) + 1  # +1 for the padding token (if needed)\n",
    "\n",
    "print(f\"词汇表大小: {vocabulary_size}\")\n",
    "print(f\"单词索引: {word_index}\")\n",
    "\n",
    "encoded_sentence = tokenizer.texts_to_sequences([segmented_sentence])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建训练数据 (CBOW 上下文-目标词对)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_pairs(corpus, window_size):\n",
    "    context_length = 2 * window_size\n",
    "    cbow_pairs = []\n",
    "    for i in range(window_size, len(corpus) - window_size):\n",
    "        context_words = [corpus[i - j] for j in range(window_size, 0, -1)]  # 前面的上下文词\n",
    "        context_words += [corpus[i + j] for j in range(1, window_size + 1)]  # 后面的上下文词\n",
    "        target_word = corpus[i]\n",
    "        cbow_pairs.append((context_words, target_word))\n",
    "    return cbow_pairs\n",
    "\n",
    "window_size = 2\n",
    "cbow_pairs = generate_cbow_pairs(encoded_sentence, window_size)\n",
    "\n",
    "# 分离上下文词和目标词\n",
    "context_words, target_words = zip(*cbow_pairs)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "context_words = np.array(context_words)\n",
    "target_words = np.array(target_words)\n",
    "\n",
    "print(f\"生成的 CBOW 词对数量: {len(cbow_pairs)}\")\n",
    "print(f\"第一个词对: context={context_words[0]}, target={target_words[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 构建 CBOW 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 构建 CBOW 模型\n",
    "embedding_dim = 50  # 词嵌入维度\n",
    "context_length = 2 * window_size # 上下文长度\n",
    "\n",
    "cbow_input = keras.Input(shape=(context_length,), dtype='int32') # 输入是上下文词序列\n",
    "embedding = layers.Embedding(vocabulary_size, embedding_dim)(cbow_input)\n",
    "average = layers.GlobalAveragePooling1D()(embedding) # 对词嵌入求平均\n",
    "output = layers.Dense(vocabulary_size, activation='softmax')(average) # 使用 softmax 进行多分类\n",
    "\n",
    "cbow = keras.Model(inputs=cbow_input, outputs=output)\n",
    "\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 编译和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 编译模型\n",
    "cbow.compile(optimizer='adam',  # Adam 优化器\n",
    "              loss='sparse_categorical_crossentropy',  # 适用于多分类问题\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 将目标词转换为 one-hot 编码\n",
    "target_words_one_hot = tf.keras.utils.to_categorical(target_words, num_classes=vocabulary_size)\n",
    "\n",
    "# 6. 训练模型\n",
    "epochs = 100\n",
    "\n",
    "cbow.fit(context_words, target_words_one_hot, epochs=epochs, batch_size=256, verbose=0)\n",
    "\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 评估 (可选) 和使用词嵌入\n",
    "\n",
    "同样，由于数据集非常小，直接评估模型的质量意义不大。  通常，你需要在一个更大的数据集上训练模型，然后评估生成的词嵌入在其他任务上的表现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 获取词嵌入\n",
    "embedding_layer = cbow.layers[1]  # Embedding 层现在是第二层\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"词嵌入矩阵的形状: {embeddings.shape}\")  # (vocabulary_size, embedding_dim)\n",
    "\n",
    "# 获取 \"计算机\" 的词嵌入\n",
    "word = \"计算机\"\n",
    "word_id = word_index[word]\n",
    "embedding_vector = embeddings[word_id]\n",
    "\n",
    "print(f\"'{word}' 的词嵌入: {embedding_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 完整代码（整合）\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "import jieba  # 用于中文分词\n",
    "\n",
    "sentence = \"我 爱 北京 天安门。 天安门 上 太阳 升。\"\n",
    "\n",
    "# 1. 中文分词\n",
    "seg_list = jieba.cut(sentence, cut_all=False)  # 使用精确模式分词\n",
    "segmented_sentence = \" \".join(seg_list)\n",
    "\n",
    "print(f\"分词结果: {segmented_sentence}\")\n",
    "\n",
    "# 2. 数据预处理\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([segmented_sentence])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = len(word_index) + 1  # +1 for the padding token (if needed)\n",
    "\n",
    "print(f\"词汇表大小: {vocabulary_size}\")\n",
    "print(f\"单词索引: {word_index}\")\n",
    "\n",
    "encoded_sentence = tokenizer.texts_to_sequences([segmented_sentence])[0]\n",
    "\n",
    "def generate_cbow_pairs(corpus, window_size):\n",
    "    context_length = 2 * window_size\n",
    "    cbow_pairs = []\n",
    "    for i in range(window_size, len(corpus) - window_size):\n",
    "        context_words = [corpus[i - j] for j in range(window_size, 0, -1)]  # 前面的上下文词\n",
    "        context_words += [corpus[i + j] for j in range(1, window_size + 1)]  # 后面的上下文词\n",
    "        target_word = corpus[i]\n",
    "        cbow_pairs.append((context_words, target_word))\n",
    "    return cbow_pairs\n",
    "\n",
    "window_size = 2\n",
    "cbow_pairs = generate_cbow_pairs(encoded_sentence, window_size)\n",
    "\n",
    "# 分离上下文词和目标词\n",
    "context_words, target_words = zip(*cbow_pairs)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "context_words = np.array(context_words)\n",
    "target_words = np.array(target_words)\n",
    "\n",
    "print(f\"生成的 CBOW 词对数量: {len(cbow_pairs)}\")\n",
    "print(f\"第一个词对: context={context_words[0]}, target={target_words[0]}\")\n",
    "\n",
    "# 4. 构建 CBOW 模型\n",
    "embedding_dim = 50  # 词嵌入维度\n",
    "context_length = 2 * window_size # 上下文长度\n",
    "\n",
    "cbow_input = keras.Input(shape=(context_length,), dtype='int32') # 输入是上下文词序列\n",
    "embedding = layers.Embedding(vocabulary_size, embedding_dim)(cbow_input)\n",
    "average = layers.GlobalAveragePooling1D()(embedding) # 对词嵌入求平均\n",
    "output = layers.Dense(vocabulary_size, activation='softmax')(average) # 使用 softmax 进行多分类\n",
    "\n",
    "cbow = keras.Model(inputs=cbow_input, outputs=output)\n",
    "\n",
    "cbow.summary()\n",
    "\n",
    "# 5. 编译模型\n",
    "cbow.compile(optimizer='adam',  # Adam 优化器\n",
    "              loss='sparse_categorical_crossentropy',  # 适用于多分类问题\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 将目标词转换为 one-hot 编码\n",
    "target_words_one_hot = tf.keras.utils.to_categorical(target_words, num_classes=vocabulary_size)\n",
    "\n",
    "# 6. 训练模型\n",
    "epochs = 100\n",
    "\n",
    "cbow.fit(context_words, target_words_one_hot, epochs=epochs, batch_size=256, verbose=0)\n",
    "\n",
    "print(\"训练完成!\")\n",
    "\n",
    "# 7. 获取词嵌入\n",
    "embedding_layer = cbow.layers[1]  # Embedding 层现在是第二层\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"词嵌入矩阵的形状: {embeddings.shape}\")  # (vocabulary_size, embedding_dim)\n",
    "\n",
    "# 获取 \"北京\" 的词嵌入\n",
    "word = \"北京\"\n",
    "word_id = word_index[word]\n",
    "embedding_vector = embeddings[word_id]\n",
    "\n",
    "print(f\"'{word}' 的词嵌入: {embedding_vector}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本 Notebook 演示了如何使用 TensorFlow/Keras 构建和训练一个中文 CBOW 模型。 关键的修改包括：\n",
    "\n",
    "*   **数据准备：** 生成 CBOW 上下文-目标词对，并对目标词进行 one-hot 编码。\n",
    "*   **模型结构：** 使用 `keras.Input` 定义输入层，接受多个上下文词。 使用 `GlobalAveragePooling1D` 对上下文词的词向量求平均。 使用 `softmax` 激活函数和 `categorical_crossentropy` 损失函数。\n",
    "*   **训练：** 将上下文词作为输入，one-hot 编码的目标词作为输出进行训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
