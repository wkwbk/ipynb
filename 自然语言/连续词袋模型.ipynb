{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文连续词袋模型 (CBOW) 实现\n\n本 Notebook 演示如何使用 Python 和 TensorFlow/Keras 构建和训练一个中文连续词袋模型 (CBOW)。\n\n**目标:**\n\n*   理解 CBOW 模型的基本原理。\n*   使用 TensorFlow/Keras 实现 CBOW 模型。\n*   准备中文训练数据。\n*   使用分词工具处理中文文本。\n*   训练模型并评估其性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "import jieba  # 用于中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备中文数据\n\n我们使用一个简单的中文句子作为示例。实际应用中，需要使用更大的中文语料库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"我 爱 北京 天安门。 天安门 上 太阳 升。\"\n",
    "\n",
    "# 1. 中文分词\n",
    "seg_list = jieba.cut(sentence, cut_all=False)  # 使用精确模式分词\n",
    "segmented_sentence = \" \".join(seg_list)\n",
    "\n",
    "print(f\"分词结果: {segmented_sentence}\")\n",
    "\n",
    "# 2. 数据预处理\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([segmented_sentence])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = len(word_index) + 1  # +1 for the padding token (if needed)\n",
    "\n",
    "print(f\"词汇表大小: {vocabulary_size}\")\n",
    "print(f\"单词索引: {word_index}\")\n",
    "\n",
    "encoded_sentence = tokenizer.texts_to_sequences([segmented_sentence])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建训练数据 (Skip-grams)\n\n和英文 CBOW 模型一样，我们使用 `skipgrams` 函数生成上下文-目标词对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 生成 Skip-gram 数据\n",
    "window_size = 2  # 上下文窗口大小\n",
    "negative_samples = 1.0  # 每个正样本的负样本数\n",
    "\n",
    "couples, labels = skipgrams(encoded_sentence, vocabulary_size, window_size=window_size, negative_samples=negative_samples)\n",
    "\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(f\"生成的词对数量: {len(couples)}\")\n",
    "print(f\"第一个词对: target={word_target[0]}, context={word_context[0]}, label={labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 构建 CBOW 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 构建 CBOW 模型\n",
    "embedding_dim = 50  # 词嵌入维度\n",
    "\n",
    "cbow = keras.models.Sequential([\n",
    "    layers.Embedding(vocabulary_size, embedding_dim, input_length=1),\n",
    "    layers.Reshape((embedding_dim,)),\n",
    "    layers.Dense(vocabulary_size, activation='sigmoid')  # 使用 softmax 进行多分类, 优化器需要调整\n",
    "])\n",
    "\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 编译和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 编译模型\n",
    "cbow.compile(optimizer='adam',  # Adam 优化器\n",
    "              loss='sparse_categorical_crossentropy',  # 适用于多分类问题\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 6. 训练模型\n",
    "epochs = 100\n",
    "\n",
    "cbow.fit([word_target, word_context], np.array(labels), epochs=epochs, batch_size=256, verbose=0)\n",
    "\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 评估 (可选) 和使用词嵌入\n\n同样，由于数据集非常小，直接评估模型的质量意义不大。  通常，你需要在一个更大的数据集上训练模型，然后评估生成的词嵌入在其他任务上的表现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 获取词嵌入\n",
    "embedding_layer = cbow.layers[0]\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"词嵌入矩阵的形状: {embeddings.shape}\")  # (vocabulary_size, embedding_dim)\n",
    "\n",
    "# 获取 \"北京\" 的词嵌入\n",
    "word = \"北京\"\n",
    "word_id = word_index[word]\n",
    "embedding_vector = embeddings[word_id]\n",
    "\n",
    "print(f\"'{word}' 的词嵌入: {embedding_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 完整代码（整合）\n\n```python\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import skipgrams\nimport jieba  # 用于中文分词\n\nsentence = \"我 爱 北京 天安门。 天安门 上 太阳 升。\"\n\n# 1. 中文分词\nseg_list = jieba.cut(sentence, cut_all=False)  # 使用精确模式分词\nsegmented_sentence = \" \".join(seg_list)\n\nprint(f\"分词结果: {segmented_sentence}\")\n\n# 2. 数据预处理\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([segmented_sentence])\n\nword_index = tokenizer.word_index\nvocabulary_size = len(word_index) + 1  # +1 for the padding token (if needed)\n\nprint(f\"词汇表大小: {vocabulary_size}\")\nprint(f\"单词索引: {word_index}\")\n\nencoded_sentence = tokenizer.texts_to_sequences([segmented_sentence])[0]\n\n# 3. 生成 Skip-gram 数据\nwindow_size = 2  # 上下文窗口大小\nnegative_samples = 1.0  # 每个正样本的负样本数\n\ncouples, labels = skipgrams(encoded_sentence, vocabulary_size, window_size=window_size, negative_samples=negative_samples)\n\nword_target, word_context = zip(*couples)\nword_target = np.array(word_target, dtype=\"int32\")\nword_context = np.array(word_context, dtype=\"int32\")\n\nprint(f\"生成的词对数量: {len(couples)}\")\nprint(f\"第一个词对: target={word_target[0]}, context={word_context[0]}, label={labels[0]}\")\n\n# 4. 构建 CBOW 模型\nembedding_dim = 50  # 词嵌入维度\n\ncbow = keras.models.Sequential([\n    layers.Embedding(vocabulary_size, embedding_dim, input_length=1),\n    layers.Reshape((embedding_dim,)),\n    layers.Dense(vocabulary_size, activation='sigmoid')  # 使用 softmax 进行多分类, 优化器需要调整\n])\n\ncbow.summary()\n\n# 5. 编译模型\ncbow.compile(optimizer='adam',  # Adam 优化器\n              loss='sparse_categorical_crossentropy',  # 适用于多分类问题\n              metrics=['accuracy'])\n\n# 6. 训练模型\nepochs = 100\n\ncbow.fit([word_target, word_context], np.array(labels), epochs=epochs, batch_size=256, verbose=0)\n\nprint(\"训练完成!\")\n\n# 7. 获取词嵌入\nembedding_layer = cbow.layers[0]\nembeddings = embedding_layer.get_weights()[0]\n\nprint(f\"词嵌入矩阵的形状: {embeddings.shape}\")  # (vocabulary_size, embedding_dim)\n\n# 获取 \"北京\" 的词嵌入\nword = \"北京\"\nword_id = word_index[word]\nembedding_vector = embeddings[word_id]\n\nprint(f\"'{word}' 的词嵌入: {embedding_vector}\")\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n\n本 Notebook 演示了如何使用 TensorFlow/Keras 构建和训练一个简单的中文 CBOW 模型。  关键区别在于需要使用中文分词工具 (例如 jieba) 对文本进行预处理。  同样，实际应用中需要更大的数据集和更复杂的模型结构。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
